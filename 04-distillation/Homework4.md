## Homework 4  

### Goals
 * Learn the basic principle of [knowledge distillation](https://arxiv.org/abs/1503.02531).
### Background
In large-scale machine learning applications, we may train a large model(teacher model) with high accuracy and transfer its 'knowledge' to a smaller model(student model) which is suitable for deployment --- this process is often called as distillation. An obvious way for distillation would be training the small model with the class probabilities produced by the large model.

* In this experiment we target the SVHN classification task in Homework1 and Homework1a. The baseline model trained in Homework1 serves as the large teacher model. 
* We aim to distill a quantized neural network, which uses discrete values for weights and features, from the baseline model. You may refer to [this paper](https://arxiv.org/abs/1606.06160) to understand how the quantization works.  
* In this experiment we recommend you set the number of bits as 2 and 1 for quantized weight and feature, respectively. You may also play with other configurations.


### Tasks

- #### Q1: Training the quantized model with ground truth labels.
  - Run the baseline code and record the test accuracy of the quantized model.

- #### Q2: Training the quantized model with the class probabilities of the teacher model.
   - Training your teacher model with the configuration of baseline in Homework1/1a.
   - Use softmax with temperature <img src="https://latex.codecogs.com/gif.latex?T&space;\geq&space;1" title="T \geq 1" /> (defined as below) to construct the 'soft' cross entropy loss with target class probabilities generated from the teacher model. Higher value of temperature T produces a softer probability distribution over classes.
Notice that the target probabilities are also generated using softmax with the same temperature.

       <img src="https://latex.codecogs.com/gif.latex?q_i&space;=&space;\frac{exp(z_i/T)}{\sum_jexp(z_j/T)}" title="q_i = \frac{exp(z_i/T)}{\sum_jexp(z_j/T)}" />

       Here <img src="https://latex.codecogs.com/gif.latex?z_i,&space;T,&space;q_i" title="z_i, T, q_i" />  denotes logits for the ith class, temperature and probability for the ith class, respectively.     
   - Rerun the training process of the quantized model with different temperature. Analyze how temperature influences the distilled model.
    #### Choose one configuration of temperature and submit your code in q2.diff.

- #### Q3: Adding hard cross entropy loss with the ground-truth labels
   -  We can add 'hard' cross entropy loss with the correct labels in the whole objective loss function as shown below. Notice that the temperature with this loss term should be set as 1. 
      
      <a href="https://www.codecogs.com/eqnedit.php?latex=Loss&space;=&space;\text{softmax-cross-entropy}(z_i,&space;gt)&space;&plus;&space;c*\text{softmax-cross-entropy}(z_i/T,&space;preds_i)" target="_blank"><img src="https://latex.codecogs.com/png.latex?Loss&space;=&space;\text{softmax-cross-entropy}(z_i,&space;gt)&space;&plus;&space;c*\text{softmax-cross-entropy}(z_i/T,&space;preds_i)" title="Loss = \text{softmax-cross-entropy}(z_i, gt) + c*\text{softmax-cross-entropy}(z_i/T, preds_i)" /></a>
     
      Here  <img src="https://latex.codecogs.com/gif.latex?preds_i" title="preds_i" /> is the  probability generated by the teacher model with temperature T.
   - We may set constant c as <img src="https://latex.codecogs.com/gif.latex?T^2" title="T^2" /> to maintain the gradients produced by the 'soft' loss term the same as that of the 'hard' loss.
   - Rerun the training process with different temperature.
     #### Choose one configuration of temperature and submit your code in q3.diff.


- #### Q4: Constructing soft loss term with ground-truth labels 
  - Instead of distilling quantized models with probabilities from teacher model,  we construct soft cross entropy loss with correct labels. So the loss term becomes as follows, 
 
     <img src="https://latex.codecogs.com/gif.latex?Loss&space;=&space;\text{softmax-cross-entropy}(z_i,&space;gt)&space;&plus;&space;c*\text{softmax-cross-entropy}(z_i/T,&space;gt)" title="Loss = \text{softmax-cross-entropy}(z_i, gt) + c*\text{softmax-cross-entropy}(z_i/T, gt)" />
  - Rerun the training process with different temperature.
    
    #### Choose one configuration of temperature and submit your code in q4.diff.
